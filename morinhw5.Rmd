---
title: "HW#5"
author: "Blain Morin"
date: "November 23, 2018"
output: pdf_document
header-includes:
- \usepackage{float}
---


```{r, echo = FALSE}

### Load Libraries

library(knitr)
library(readr)
library(MASS)
library(leaps)
library(dplyr)
library(stargazer)
library(caret)
library(extrafont)
library(glmnet)

```

# Question 1:

```{r, echo = FALSE, message = FALSE, warning = FALSE}

### Load data
iod = read_csv("iodatadev.csv")

### Filter out columns with more that 10% missing data
iod.clean = Filter(function(x) mean(is.na(x)) < 0.1, iod)

### Get complete cases
iod.clean = iod.clean %>%
  filter(complete.cases(.))

### Filter out ids
iod.clean = iod.clean %>%
  select(-X1, -ID, -"..2", -"..2_1", -WEIGHT, -HEIGHT, -c.cys, -drds, -mayodonor, -rass1, -CSG, -gronigendonor, - ccpdonor, - c.scr, - mayo)

```



```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}

# ### Forward steps
# forward = regsubsets(GFR~., data = iod.clean, nvmax=26, method="forward")
# forward.summary = summary(forward)
# forward.summary$rsq
# forward.summary$

set.seed(1)

### Forward and backward step regression
full = lm(GFR ~ ., iod.clean)
base = lm(GFR ~ 1, iod.clean)
fulltest = stepAIC(full, scope = list(upper=full,lower=base), direction = "both", trace = FALSE)
summary.fulltest = summary(fulltest)
rsquared.fulltest = summary.fulltest$r.squared


### Bootstrap 1000 samples and get rsquares
set.seed(1)
nsims = 1000

r2boot.train = rep(NA, nsims)
r2boot.test = rep(NA, nsims)

for (i in 1:nsims) {
  
  train = sample_n(iod.clean, size = nrow(iod.clean), replace = TRUE)
  test = anti_join(iod.clean, train)
  mod = stepAIC(lm(GFR ~ ., data = train), direction = "both", trace = FALSE)
  summary.mod = summary(mod)
  r2boot.train[i] = summary.mod$r.squared
  preds = predict(mod, test)
  tss = sum((test$GFR - mean(test$GFR))^2)
  ess = sum((test$GFR - preds)^2)
  r2boot.test[i] = 1 - (ess/tss)
  
  
}


### Calculate average optimism
optimism = r2boot.train - r2boot.test
ave.optimism = mean(optimism)

### Calculate Estimated Test r2
est.test.r2 = rsquared.fulltest - ave.optimism 



```


# Question 2:


## Step CV:

```{r, echo = FALSE, warning = TRUE, message = FALSE}

### Function to get predictions for leap package
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

### Crossvalidate number of variables for step

set.seed(15)

folds = 10

test = createFolds(iod.clean$GFR, k = folds)

cv.errors = matrix(NA, nrow = folds, ncol = ncol(iod.clean) - 1 )
step.r2.train = matrix(NA, nrow = folds, ncol = ncol(iod.clean) - 1)
step.r2.test = matrix(NA, nrow = folds, ncol = ncol(iod.clean) - 1)

for (i in 1:folds) {
  
  training = iod.clean[-test[[i]], ]
  testing = iod.clean[test[[i]], ]
  best.model = regsubsets(GFR ~ .,
                          data = training,
                          nvmax = ncol(iod.clean),
                          method = "seqrep")
  
  for (j in 1:(ncol(iod.clean)-1)) {
    
    preds.training = predict.regsubsets(best.model, newdata = training, id = j)
    preds = predict.regsubsets(best.model, newdata = testing, id = j)
    
    
    #R2 for training folds
    train.tss = sum((training$GFR - mean(training$GFR))^2)
    train.ess = sum((training$GFR - preds.training)^2)
    r2train =  1 - (train.ess/train.tss)
    step.r2.train[i,j] = r2train
    
    
    #R2 for testing folds
    test.tss = sum((testing$GFR - mean(testing$GFR))^2)
    test.ess = sum((testing$GFR - preds)^2)
    r2test =  1 - (test.ess/test.tss)
    step.r2.test[i,j] = r2test
    
    
    
    MSE = mean((testing$GFR - preds)^2)
    cv.errors[i, j] = MSE
    
    
    
  }
  
  
}


mean.cv.errors = apply(cv.errors, 2, mean)

step.plot.frame = data.frame(variables = seq(1:(ncol(iod.clean)-1)), error = mean.cv.errors)

step.plot = step.plot.frame %>%
  ggplot(aes(x = variables, y = mean.cv.errors)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))


### Use best model on full data

which.min(mean.cv.errors) ## 12 variables is best

### Run models
final.step = regsubsets(GFR ~ .,
                          data = iod.clean,
                          nvmax = ncol(iod.clean),
                          method = "seqrep")

### Extract 12 variable model's rsquared
final.step.r2 = summary(final.step)$rsq[which.min(mean.cv.errors)]

### Adjust the r2 by the optimism
step.optimisms = step.r2.train - step.r2.test
mean.step.optimisms = apply(step.optimisms, 2, mean)

### Adjusted r2
step.adj.r2 = final.step.r2 - mean.step.optimisms[which.min(mean.cv.errors)]

  
```

## Ridge

```{r}

set.seed(11)

folds = 10

### Create folds indexes for the observations left out
test = createFolds(iod.clean$GFR, k = folds)

### Grid of lambdas to check
grid=seq(1, .001, length=100)


### Set up data

x = model.matrix(GFR ~ . - 1, data = iod.clean)
y = iod.clean$GFR


### Initialize matrices for mse, r2train, r2test
ridge.cv.errors = matrix(NA, nrow = folds, ncol = length(grid))
ridge.r2.train = matrix(NA, nrow = folds, ncol = length(grid))
ridge.r2.test = matrix(NA, nrow = folds, ncol = length(grid))


### For each fold, make test and training x and y
### For each lambda, calculate r2 and mse
for (i in 1:folds) {
  
  trainingx = x[-test[[i]], ]
  trainingy = y[-test[[i]]]
  testingx = x[test[[i]], ]
  testingy = y[test[[i]]]
  
  for (j in 1:length(grid)){
    
    best.model = glmnet(x = trainingx, y = trainingy, alpha = 0, lambda = grid)
    
    trainpreds = predict(best.model, newx = trainingx, s = grid[j])
    preds = predict(best.model, newx = testingx, s = grid[j])
    
    
    #R2 for training folds
    train.tss = sum((trainingy - mean(trainingy))^2)
    train.ess = sum((trainingy - trainpreds)^2)
    r2train =  1 - (train.ess/train.tss)
    ridge.r2.train[i,j] = r2train
    
    
    #R2 for testing folds
    test.tss = sum((testingy - mean(testingy))^2)
    test.ess = sum((testingy - preds)^2)
    r2test =  1 - (test.ess/test.tss)
    ridge.r2.test[i,j] = r2test
    
    #MSE 
    MSE = mean((preds - testingy)^2)
    ridge.cv.errors[i, j] = MSE
    
  }
  
}


### Get average Optimism for each lambda
ridge.optimism = ridge.r2.train - ridge.r2.test
est.ridge.optimism = apply(ridge.optimism, 2, mean)

### Get average MSE for each lambda
mean.ridge.errors = apply(ridge.cv.errors, 2, mean)

ridge.plot.frame = data.frame(lambda = grid, error = mean.ridge.errors)

ridge.plot = ridge.plot.frame %>% ggplot(aes(x = lambda, y = error)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

### Get min lambda and its optimism
ridge.plot.frame[which.min(ridge.plot.frame$error), ]
best.ridge.opt = est.ridge.optimism[which.min(ridge.plot.frame$error)]


### Fit model with best lambda
final.ridge = glmnet(x = x, y = y, alpha = 0, lambda = grid[which.min(ridge.plot.frame$error)])

### Get final model preds
final.ridge.preds = predict(final.ridge, newx = x)


### Get final model r2
final.ridge.tss = sum((y - mean(y))^2)
final.ridge.ess = sum((y - final.ridge.preds)^2)
final.ridge.r2 =  1 - (final.ridge.ess/final.ridge.tss)


### Get adj final model r2
final.ridge.adj.r2 = final.ridge.r2 - best.ridge.opt

```

## Lasso

```{r}


set.seed(11)

folds = 10

### Create folds makes indexes for left out observations
test = createFolds(iod.clean$GFR, k = folds)

### Grid of lambdas to check
grid=seq(1, .001, length=100)


### Set up data

x = model.matrix(GFR ~ . - 1, data = iod.clean)
y = iod.clean$GFR


### Initialize matrices for mse, r2train, r2test
lasso.cv.errors = matrix(NA, nrow = folds, ncol = length(grid))
lasso.r2.train = matrix(NA, nrow = folds, ncol = length(grid))
lasso.r2.test = matrix(NA, nrow = folds, ncol = length(grid))


### For each fold, make test and training x and y
### For each lambda, calculate r2 and mse
for (i in 1:folds) {
  
  trainingx = x[-test[[i]], ]
  trainingy = y[-test[[i]]]
  testingx = x[test[[i]], ]
  testingy = y[test[[i]]]
  
  for (j in 1:length(grid)){
    
    best.model = glmnet(x = trainingx, y = trainingy, alpha = 1, lambda = grid)
    
    trainpreds = predict(best.model, newx = trainingx, s = grid[j])
    preds = predict(best.model, newx = testingx, s = grid[j])
    
    
    #R2 for training folds
    train.tss = sum((trainingy - mean(trainingy))^2)
    train.ess = sum((trainingy - trainpreds)^2)
    r2train =  1 - (train.ess/train.tss)
    lasso.r2.train[i,j] = r2train
    
    
    #R2 for testing folds
    test.tss = sum((testingy - mean(testingy))^2)
    test.ess = sum((testingy - preds)^2)
    r2test =  1 - (test.ess/test.tss)
    lasso.r2.test[i,j] = r2test
    
    #MSE 
    MSE = mean((preds - testingy)^2)
    lasso.cv.errors[i, j] = MSE
    
  }
  
}


### Get average Optimism for each lambda
lasso.optimism = lasso.r2.train - lasso.r2.test
est.lasso.optimism = apply(lasso.optimism, 2, mean)

### Get average MSE for each lambda
mean.lasso.errors = apply(lasso.cv.errors, 2, mean)

lasso.plot.frame = data.frame(lambda = grid, error = mean.lasso.errors)

lasso.plot.frame %>% ggplot(aes(x = lambda, y = error)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

### Get min lambda and its optimism
lasso.plot.frame[which.min(lasso.plot.frame$error), ]
best.lasso.opt = est.lasso.optimism[which.min(lasso.plot.frame$error)]

### Fit model with best lambda
final.lasso = glmnet(x = x, y = y, alpha = 1, lambda = grid[which.min(lasso.plot.frame$error)])

### Get final model preds
final.lasso.preds = predict(final.lasso, newx = x)


### Get final model r2
final.lasso.tss = sum((y - mean(y))^2)
final.lasso.ess = sum((y - final.lasso.preds)^2)
final.lasso.r2 =  1 - (final.lasso.ess/final.ridge.tss)


### Get adj final model r2
final.lasso.adj.r2 = final.lasso.r2 - best.lasso.opt


```


```{r}

forkable = data.frame(step = step.adj.r2, ridge = final.ridge.adj.r2, lasso = final.lasso.adj.r2)
kable(forkable)

```